#!/usr/bin/env python3
"""
EchoTranscribe Backend - FastAPI server for audio transcription
Author: paladini (https://github.com/paladini)
Repository: https://github.com/paladini/echo-transcribe
"""

import os
import asyncio
import tempfile
import shutil
import socket
from pathlib import Path
from typing import List, Optional
import uvicorn
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def find_available_port(start_port=8000, max_attempts=10):
    for port in range(start_port, start_port + max_attempts):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('127.0.0.1', port))
                return port
        except OSError:
            continue
    return None

# Inicializar FastAPI
app = FastAPI(
    title="EchoTranscribe API",
    description="API para transcri√ß√£o de √°udio usando modelos locais",
    version="0.1.0"
)

# Servir arquivos est√°ticos do frontend React
FRONTEND_DIR = Path(__file__).parent / "frontend"
if FRONTEND_DIR.exists():
    app.mount("/", StaticFiles(directory=str(FRONTEND_DIR), html=True), name="frontend")

@app.middleware("http")
async def spa_fallback(request: Request, call_next):
    # Se n√£o for rota de API e n√£o existir arquivo, retorna index.html
    response = await call_next(request)
    if response.status_code == 404 and not request.url.path.startswith("/api"):
        index_path = FRONTEND_DIR / "index.html"
        if index_path.exists():
            return FileResponse(str(index_path))
    return response

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:1420", "tauri://localhost"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Modelos de dados
class TranscriptionResponse(BaseModel):
    text: str
    confidence: Optional[float] = None
    processing_time: Optional[float] = None
    detected_language: Optional[str] = None
    word_timestamps: Optional[List[dict]] = None

class ModelInfo(BaseModel):
    name: str
    size: str
    description: str
    available: bool

class BatchTranscriptionResponse(BaseModel):
    results: List[dict]
    total_files: int
    successful: int
    failed: int

class BatchTranscriptionRequest(BaseModel):
    files: List[str]  # Lista de nomes de arquivos tempor√°rios
    model: str
    language: Optional[str] = None

# Vari√°veis globais
MODELS_DIR = Path.home() / ".echo-transcribe" / "models"
TEMP_DIR = Path.home() / ".echo-transcribe" / "temp"

# Criar diret√≥rios se n√£o existirem
MODELS_DIR.mkdir(parents=True, exist_ok=True)
TEMP_DIR.mkdir(parents=True, exist_ok=True)

# Lista de modelos dispon√≠veis
AVAILABLE_MODELS = [
    ModelInfo(
        name="tiny",
        size="39 MB",
        description="Modelo pequeno e r√°pido, menor precis√£o",
        available=False
    ),
    ModelInfo(
        name="base",
        size="74 MB", 
        description="Modelo balanceado entre velocidade e precis√£o",
        available=False
    ),
    ModelInfo(
        name="small",
        size="244 MB",
        description="Modelo com boa precis√£o, velocidade m√©dia",
        available=False
    ),
    ModelInfo(
        name="medium",
        size="769 MB",
        description="Modelo com alta precis√£o, mais lento",
        available=False
    )
]

def check_model_availability():
    """Verifica quais modelos est√£o dispon√≠veis localmente"""
    for model in AVAILABLE_MODELS:
        model_path = MODELS_DIR / f"whisper-{model.name}"
        model.available = model_path.exists()

# Vari√°vel para armazenar o modelo carregado
current_model = None
current_model_name = None

def load_whisper_model(model_name: str):
    """Carrega o modelo Whisper especificado"""
    global current_model, current_model_name
    
    try:
        # Importar faster-whisper apenas quando necess√°rio
        from faster_whisper import WhisperModel
        
        if current_model_name == model_name and current_model is not None:
            logger.info(f"Modelo {model_name} j√° carregado")
            return current_model
            
        logger.info(f"Carregando modelo {model_name}...")
        model_path = MODELS_DIR / f"whisper-{model_name}"
        
        if not model_path.exists():
            # Baixar modelo se n√£o existir
            logger.info(f"Baixando modelo {model_name}...")
            current_model = WhisperModel(model_name, download_root=str(MODELS_DIR))
        else:
            current_model = WhisperModel(str(model_path))
            
        current_model_name = model_name
        logger.info(f"Modelo {model_name} carregado com sucesso")
        return current_model
        
    except ImportError:
        logger.error("faster-whisper n√£o est√° instalado")
        raise HTTPException(
            status_code=500, 
            detail="Biblioteca de transcri√ß√£o n√£o encontrada. Instale faster-whisper."
        )
    except Exception as e:
        logger.error(f"Erro ao carregar modelo {model_name}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao carregar modelo: {str(e)}"
        )

@app.get("/")
async def root():
    """Endpoint raiz para verificar se a API est√° funcionando"""
    return {"message": "EchoTranscribe API est√° funcionando!", "version": "0.1.0"}

@app.get("/health")
async def health_check():
    """Endpoint de health check"""
    return {"status": "healthy", "timestamp": asyncio.get_event_loop().time()}

@app.get("/models", response_model=List[ModelInfo])
async def get_models():
    """Lista todos os modelos dispon√≠veis"""
    check_model_availability()
    return AVAILABLE_MODELS

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    model: str = "base",
    language: Optional[str] = None,
    auto_detect_language: bool = True
):
    """
    Transcreve um arquivo de √°udio
    
    Args:
        file: Arquivo de √°udio (MP3, WAV, FLAC, M4A)
        model: Nome do modelo a ser usado (tiny, base, small, medium)
        language: C√≥digo do idioma (opcional, auto-detecta se n√£o especificado)
        auto_detect_language: Se deve detectar automaticamente o idioma
    """
    
    # Validar formato do arquivo
    allowed_extensions = {'.mp3', '.wav', '.flac', '.m4a', '.ogg', '.webm'}
    file_extension = Path(file.filename).suffix.lower()
    
    if file_extension not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"Formato de arquivo n√£o suportado: {file_extension}. "
                   f"Formatos aceitos: {', '.join(allowed_extensions)}"
        )
    
    # Validar modelo
    valid_models = [m.name for m in AVAILABLE_MODELS]
    if model not in valid_models:
        raise HTTPException(
            status_code=400,
            detail=f"Modelo inv√°lido: {model}. Modelos dispon√≠veis: {', '.join(valid_models)}"
        )
    
    # Criar arquivo tempor√°rio
    temp_file = None
    try:
        # Salvar arquivo tempor√°rio
        temp_file = tempfile.NamedTemporaryFile(
            delete=False, 
            suffix=file_extension,
            dir=str(TEMP_DIR)
        )
        
        # Copiar conte√∫do do upload para o arquivo tempor√°rio
        shutil.copyfileobj(file.file, temp_file)
        temp_file.close()
        
        logger.info(f"Arquivo tempor√°rio criado: {temp_file.name}")
        
        # Carregar modelo
        whisper_model = load_whisper_model(model)
        
        # Realizar transcri√ß√£o
        logger.info(f"Iniciando transcri√ß√£o com modelo {model}")
        start_time = asyncio.get_event_loop().time()
        
        # Se auto_detect_language for True e language n√£o foi especificado, detectar idioma
        detected_language = None
        if auto_detect_language and not language:
            logger.info("Detectando idioma automaticamente...")
            # Usar apenas os primeiros 30 segundos para detec√ß√£o de idioma
            segments, info = whisper_model.transcribe(
                temp_file.name,
                language=None,  # Deixar o modelo detectar
                beam_size=1,    # Usar beam size menor para ser mais r√°pido
                best_of=1,
                temperature=0.0,
                condition_on_previous_text=False,
                word_timestamps=False
            )
            detected_language = info.language
            logger.info(f"Idioma detectado: {detected_language}")
        
        # Transcri√ß√£o completa com idioma detectado ou especificado
        final_language = language or detected_language
        segments, info = whisper_model.transcribe(
            temp_file.name,
            language=final_language,
            beam_size=5,
            best_of=5,
            temperature=0.0,
            word_timestamps=True,  # Habilitar timestamps por palavra
            condition_on_previous_text=False
        )
        
        # Concatenar segmentos e coletar timestamps
        transcription_text = ""
        word_timestamps = []
        
        for segment in segments:
            transcription_text += segment.text + " "
            # Coletar timestamps de palavras se dispon√≠veis
            if hasattr(segment, 'words') and segment.words:
                for word in segment.words:
                    word_timestamps.append({
                        "word": word.word,
                        "start": word.start,
                        "end": word.end,
                        "probability": getattr(word, 'probability', None)
                    })
        
        end_time = asyncio.get_event_loop().time()
        processing_time = end_time - start_time
        
        logger.info(f"Transcri√ß√£o conclu√≠da em {processing_time:.2f} segundos")
        
        # Agendar limpeza do arquivo tempor√°rio
        background_tasks.add_task(cleanup_temp_file, temp_file.name)
        
        return TranscriptionResponse(
            text=transcription_text.strip(),
            confidence=None,  # faster-whisper n√£o fornece confidence score diretamente
            processing_time=processing_time,
            detected_language=detected_language,
            word_timestamps=word_timestamps
        )
        
    except Exception as e:
        logger.error(f"Erro durante transcri√ß√£o: {str(e)}")
        # Limpar arquivo tempor√°rio em caso de erro
        if temp_file and os.path.exists(temp_file.name):
            os.unlink(temp_file.name)
        
        raise HTTPException(
            status_code=500,
            detail=f"Erro durante transcri√ß√£o: {str(e)}"
        )

@app.post("/transcribe-batch", response_model=BatchTranscriptionResponse)
async def transcribe_batch(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...),
    model: str = "base",
    language: Optional[str] = None,
    auto_detect_language: bool = True
):
    """
    Transcreve m√∫ltiplos arquivos de √°udio em lote
    
    Args:
        files: Lista de arquivos de √°udio
        model: Nome do modelo a ser usado
        language: C√≥digo do idioma (opcional)
        auto_detect_language: Se deve detectar automaticamente o idioma
    """
    
    if len(files) > 10:  # Limitar a 10 arquivos por vez
        raise HTTPException(
            status_code=400,
            detail="M√°ximo de 10 arquivos por requisi√ß√£o"
        )
    
    results = []
    successful = 0
    failed = 0
    
    for file in files:
        try:
            # Validar formato do arquivo
            allowed_extensions = {'.mp3', '.wav', '.flac', '.m4a', '.ogg', '.webm'}
            file_extension = Path(file.filename).suffix.lower()
            
            if file_extension not in allowed_extensions:
                results.append({
                    "filename": file.filename,
                    "status": "error",
                    "error": f"Formato n√£o suportado: {file_extension}",
                    "text": "",
                    "processing_time": 0
                })
                failed += 1
                continue
            
            # Criar arquivo tempor√°rio
            temp_file = tempfile.NamedTemporaryFile(
                delete=False, 
                suffix=file_extension,
                dir=str(TEMP_DIR)
            )
            
            # Copiar conte√∫do do upload
            shutil.copyfileobj(file.file, temp_file)
            temp_file.close()
            
            # Carregar modelo
            whisper_model = load_whisper_model(model)
            
            # Transcrever
            start_time = asyncio.get_event_loop().time()
            
            # Detectar idioma se necess√°rio
            detected_language = None
            if auto_detect_language and not language:
                segments, info = whisper_model.transcribe(
                    temp_file.name,
                    language=None,
                    beam_size=1,
                    best_of=1,
                    temperature=0.0,
                    condition_on_previous_text=False,
                    word_timestamps=False
                )
                detected_language = info.language
            
            # Transcri√ß√£o completa
            final_language = language or detected_language
            segments, info = whisper_model.transcribe(
                temp_file.name,
                language=final_language,
                beam_size=5,
                best_of=5,
                temperature=0.0,
                word_timestamps=True,
                condition_on_previous_text=False
            )
            
            # Processar resultado
            transcription_text = ""
            word_timestamps = []
            
            for segment in segments:
                transcription_text += segment.text + " "
                if hasattr(segment, 'words') and segment.words:
                    for word in segment.words:
                        word_timestamps.append({
                            "word": word.word,
                            "start": word.start,
                            "end": word.end,
                            "probability": getattr(word, 'probability', None)
                        })
            
            end_time = asyncio.get_event_loop().time()
            processing_time = end_time - start_time
            
            results.append({
                "filename": file.filename,
                "status": "completed",
                "text": transcription_text.strip(),
                "processing_time": processing_time,
                "detected_language": detected_language,
                "word_timestamps": word_timestamps
            })
            
            successful += 1
            
            # Agendar limpeza
            background_tasks.add_task(cleanup_temp_file, temp_file.name)
            
        except Exception as e:
            logger.error(f"Erro ao transcrever {file.filename}: {str(e)}")
            results.append({
                "filename": file.filename,
                "status": "error",
                "error": str(e),
                "text": "",
                "processing_time": 0
            })
            failed += 1
            
            # Limpar arquivo tempor√°rio em caso de erro
            if 'temp_file' in locals() and temp_file and os.path.exists(temp_file.name):
                os.unlink(temp_file.name)
    
    return BatchTranscriptionResponse(
        results=results,
        total_files=len(files),
        successful=successful,
        failed=failed
    )

async def cleanup_temp_file(file_path: str):
    """Remove arquivo tempor√°rio"""
    try:
        if os.path.exists(file_path):
            os.unlink(file_path)
            logger.info(f"Arquivo tempor√°rio removido: {file_path}")
    except Exception as e:
        logger.warning(f"Erro ao remover arquivo tempor√°rio {file_path}: {str(e)}")

@app.on_event("startup")
async def startup_event():
    """Evento executado na inicializa√ß√£o da API"""
    logger.info("EchoTranscribe API iniciada")
    check_model_availability()

@app.on_event("shutdown")
async def shutdown_event():
    """Evento executado no encerramento da API"""
    logger.info("EchoTranscribe API encerrada")
    
    # Limpar arquivos tempor√°rios
    try:
        for temp_file in TEMP_DIR.glob("*"):
            if temp_file.is_file():
                temp_file.unlink()
        logger.info("Arquivos tempor√°rios limpos")
    except Exception as e:
        logger.warning(f"Erro ao limpar arquivos tempor√°rios: {str(e)}")

if __name__ == "__main__":
    # Verificar se todas as depend√™ncias est√£o instaladas
    try:
        import faster_whisper
        import torch
        logger.info("‚úÖ All dependencies are installed")
    except ImportError as e:
        logger.error(f"‚ùå Missing dependency: {e}")
        logger.error("Please install dependencies with: pip install -r requirements.txt")
        exit(1)
    
    # Verificar disponibilidade do PyTorch
    try:
        if torch.cuda.is_available():
            logger.info("üöÄ GPU (CUDA) available for acceleration")
        else:
            logger.info("‚ö†Ô∏è Using CPU only (no CUDA available)")
    except Exception as e:
        logger.warning(f"Could not check CUDA availability: {e}")
    
    # Criar diret√≥rio de modelos se n√£o existir
    try:
        MODELS_DIR.mkdir(parents=True, exist_ok=True)
        TEMP_DIR.mkdir(parents=True, exist_ok=True)
        logger.info("üìÅ Directories created successfully")
    except Exception as e:
        logger.error(f"‚ùå Could not create directories: {e}")
        exit(1)
    
    logger.info("üéôÔ∏è Starting EchoTranscribe backend server...")
    
    # Encontrar porta dispon√≠vel
    port = find_available_port(8000, 5)  # Tentar portas 8000-8004
    if port is None:
        logger.error("‚ùå No available ports found in range 8000-8004")
        exit(1)
    
    if port != 8000:
        logger.warning(f"‚ö†Ô∏è Port 8000 is busy, using port {port} instead")
    
    # Salvar porta para o frontend
    try:
        port_file = Path(__file__).parent / "backend_port.txt"
        port_file.write_text(str(port))
        logger.info(f"üìù Port {port} saved to backend_port.txt")
    except Exception as e:
        logger.warning(f"Could not save port file: {e}")
    
    logger.info(f"üåê Backend will be available at: http://127.0.0.1:{port}")
    logger.info(f"üìö API docs will be available at: http://127.0.0.1:{port}/docs")
    
    try:
        # Configura√ß√£o para produ√ß√£o e desenvolvimento
        uvicorn.run(
            "main:app",
            host="127.0.0.1",
            port=port,
            reload=False,  # Desabilitado para produ√ß√£o
            log_level="info",
            access_log=True
        )
    except OSError as e:
        if "Address already in use" in str(e) or "address already in use" in str(e):
            logger.error("‚ùå Port 8000 is already in use. Another instance may be running.")
            logger.error("üí° Try closing other instances or wait a few seconds and try again.")
            exit(2)  # Exit code 2 for port conflict
        else:
            logger.error(f"‚ùå Failed to start server (OSError): {e}")
            exit(1)
    except KeyboardInterrupt:
        logger.info("üëã Server stopped by user")
    except Exception as e:
        logger.error(f"‚ùå Failed to start server: {e}")
        exit(1)
